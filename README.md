# ğŸ§  RAG Knowledge Assistant

A **Retrieval-Augmented Generation (RAG)**-based intelligent assistant
built using **React + TypeScript** for the frontend, **FastAPI** for the
backend, and **LLaMA** from **Hugging Face** as the open-source language
model.\
It integrates **vector-based document retrieval** with **natural
language understanding** to answer user queries based on your knowledge
base.

------------------------------------------------------------------------

## ğŸš€ Features

-   âš¡ **Real-time Q&A** using RAG pipeline
-   ğŸ” **Semantic Search** powered by vector embeddings
-   ğŸ§  **LLaMA Integration** (Hugging Face open-source model)
-   ğŸ¨ **Modern UI** built with **React + Tailwind CSS**
-   ğŸŒ **REST APIs** using FastAPI backend
-   ğŸ“„ **Multi-format document ingestion** (PDF, DOCX, TXT)

------------------------------------------------------------------------

## ğŸ› ï¸ Tech Stack

  Component       Technology
  --------------- ----------------------------------
  **Frontend**    React + TypeScript + TailwindCSS
  **Backend**     FastAPI (Python)
  **Vector DB**   FAISS
  **LLM**         Hugging Face LLaMA
  **Embedding**   Hugging Face Transformers
  **Styling**     Tailwind CSS

------------------------------------------------------------------------

## ğŸ§© Architecture

``` mermaid
flowchart TD
    A[User Query] --> B[React + Tailwind Frontend]
    B --> C[FastAPI Backend]
    C --> D[Embedding Generator]
    D --> E[Vector DB - FAISS]
    E --> F[Relevant Docs]
    C --> G[LLaMA Model - Hugging Face]
    F --> G
    G --> H[Final Answer Returned]
    H --> A
```

------------------------------------------------------------------------

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

``` bash
git clone https://github.com/your-username/rag-knowledge-assistant.git
cd rag-knowledge-assistant
```

### 2ï¸âƒ£ Install dependencies

#### Frontend

``` bash
cd frontend
npm install
```

#### Backend

``` bash
cd backend
pip install -r requirements.txt
```

### 3ï¸âƒ£ Set up environment variables

Create a `.env` file in your **backend** folder:

``` env
HUGGINGFACE_API_KEY=your_huggingface_token
MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
VECTOR_DB_PATH=./vector_store
```

------------------------------------------------------------------------

## â–¶ï¸ Run the Project

### **Backend**

``` bash
cd backend
uvicorn app:app --reload
```

### **Frontend**

``` bash
cd frontend
npm run dev
```

Open: **http://localhost:5173**

## Create Virtual Environment

``` bash
python -m venv venv
venv\Scripts\activate
```

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    rag-knowledge-assistant/
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ main.py                # FastAPI app
    â”‚   â”œâ”€â”€ ingestion.py           # Document ingestion pipeline
    â”‚   â”œâ”€â”€ retrieval.py           # RAG + LLaMA integration
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â””â”€â”€ .env
    â”œâ”€â”€ frontend/
    â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”œâ”€â”€ components/        # Chat UI components
    â”‚   â”‚   â”œâ”€â”€ App.tsx
    â”‚   â”‚   â””â”€â”€ index.css
    â”‚   â”œâ”€â”€ package.json
    â”‚   â””â”€â”€ vite.config.ts
    â””â”€â”€ README.md

------------------------------------------------------------------------

## ğŸ”® Future Enhancements

-   ğŸ—‚ **Multi-file document ingestion**
-   ğŸ§© **Support for OpenAI + Mixtral models**
-   ğŸ“Š **Analytics dashboard**
-   ğŸŒ **Deployment-ready Docker setup**

------------------------------------------------------------------------

## ğŸ¤ Contributing

Pull requests are welcome! For significant changes, please open an issue
first.
